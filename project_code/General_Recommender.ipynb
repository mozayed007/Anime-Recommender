{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e07a14e7",
      "metadata": {
        "id": "e07a14e7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# **1-Dataset Analysis:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d67f80f",
      "metadata": {
        "id": "3d67f80f",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## *i) Cleaning Dataset:* <br />\n",
        "   Jan 23 Last edits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc326491",
      "metadata": {
        "id": "bc326491"
      },
      "source": [
        "Latest check 30th of Jan. 2023 at 5:00PM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3dcbc79",
      "metadata": {
        "id": "e3dcbc79",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### **Importings:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b1879af-3046-4b04-ba44-5921bd475314",
      "metadata": {
        "id": "8b1879af-3046-4b04-ba44-5921bd475314",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.express as px\n",
        "from ast import literal_eval\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa08999a",
      "metadata": {
        "id": "aa08999a",
        "outputId": "95aafdc4-e06b-4dd4-b2f2-8342e14cdc59",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data= pd.read_csv(\"../data-history/up-to-date-MAL/anime_Feb23.csv\")\n",
        "\n",
        "\n",
        "print(data.shape)\n",
        "data.head(5).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65024cbe",
      "metadata": {
        "id": "65024cbe",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Taking care of nulls and drops:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9875ecd",
      "metadata": {
        "id": "e9875ecd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "drops=[\"main_picture_medium\",\"main_picture_large\",\"broadcast_day_of_the_week\",\"broadcast_start_time\",\"alternative_titles_en\",\"alternative_titles_ja\",\"alternative_titles_synonyms\"]\n",
        "data['fav_percent'] = data['num_favorites'] / data['num_list_users']\n",
        "data_main=data.drop(drops,axis=1)[['id','title','media_type','mean','num_scoring_users','num_episodes',\"source\",'popularity','fav_percent','rank','rating',\"genres\",\"studios\",'synopsis',\"nsfw\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8134a3f6",
      "metadata": {
        "id": "8134a3f6",
        "outputId": "9699a8e7-11ff-4610-d112-04874aec5cd9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data_main.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4707bde4",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_main[data_main.synopsis.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8e1def6",
      "metadata": {},
      "outputs": [],
      "source": [
        "(df.groupby('media_type')['media_type'].count()/data_main.groupby('media_type')['media_type'].count()).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b30c56",
      "metadata": {},
      "outputs": [],
      "source": [
        "(df.groupby('source')['source'].count()/data_main.groupby('source')['source'].count()).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00324d5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "(df.groupby('rating')['rating'].count()/data_main.groupby('rating')['rating'].count()).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c50c434",
      "metadata": {},
      "outputs": [],
      "source": [
        "(df.groupby('nsfw')['nsfw'].count()/data_main.groupby('nsfw')['nsfw'].count()).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9faa1ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_na(df,col):\n",
        "    index = df[df[col].isna()].index\n",
        "    value = np.abs(np.random.normal(loc=df[col].mean(), scale=df[col].std(), size=df[col].isna().sum()))\n",
        "    df[col] = df[col].fillna(pd.Series(value, index=index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d0b3e6",
      "metadata": {
        "id": "c4d0b3e6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fill_na(data_main,'mean')\n",
        "fill_na(data_main,'rank')\n",
        "data_main['fav_percent'] = data_main['fav_percent'].fillna(value=round(data_main['fav_percent'].mean(),3))\n",
        "data_main['num_episodes'] = data_main['num_episodes'].fillna(value=round(data_main['num_episodes'].mean()))\n",
        "data_main['source'] = data_main['source'].fillna(value=data_main['source'].mode()[0])\n",
        "data_main['rating'] = data_main['rating'].fillna(value=data_main['rating'].mode()[0])\n",
        "data_main['synopsis'] = data_main['synopsis'].fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "061316ef",
      "metadata": {
        "id": "061316ef",
        "outputId": "1a48fc32-9820-4c80-bcf1-d01b928fa261"
      },
      "outputs": [],
      "source": [
        "data_main.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0a376e",
      "metadata": {
        "id": "df0a376e",
        "outputId": "4e57dba3-68be-400d-f86b-a7dcf8d354a0"
      },
      "outputs": [],
      "source": [
        "data_main.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af7069e",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_main.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b047a3d",
      "metadata": {
        "id": "7b047a3d",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c33b9a21",
      "metadata": {
        "id": "c33b9a21",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm.notebook import tqdm\n",
        "import ast\n",
        "import re\n",
        "import spacy as sp\n",
        "from keybert import KeyBERT\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf05961",
      "metadata": {
        "id": "ccf05961",
        "outputId": "29d4eee1-a44a-49f6-e368-5015ef204cf9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data_main.title.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915f4335",
      "metadata": {
        "id": "915f4335",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## *ii)EDA:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa6177b",
      "metadata": {
        "id": "9fa6177b",
        "outputId": "c2191875-b365-4e34-89db-873939916877",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data_main.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11afdf8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_main['mean'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811268aa",
      "metadata": {
        "id": "811268aa",
        "outputId": "dfff3dde-1360-4e69-aecc-45e9233c751b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"dark\")\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.hist(data['mean'], bins=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0966f7ab",
      "metadata": {
        "id": "0966f7ab",
        "outputId": "bc16d940-38f0-4806-cdda-2ad4b06bd903",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fig = px.pie(data_main, 'media_type')\n",
        "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da747588",
      "metadata": {
        "id": "da747588",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "*Notes:* Naturally TV has higher percentage as anime media.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4782830d",
      "metadata": {
        "id": "4782830d",
        "outputId": "b0451208-be14-44d0-a866-7c76d296c392",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "corr = data_main.corr()\n",
        "\n",
        "# Set up the matplotlib plot configuration\n",
        "#\n",
        "f, ax = plt.subplots(figsize=(16, 10))\n",
        "#\n",
        "# Generate a mask for upper traingle\n",
        "#\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "#\n",
        "# Configure a custom diverging colormap\n",
        "#\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "#\n",
        "# Draw the heatmap\n",
        "#\n",
        "sns.heatmap(corr, annot=True, mask = mask, cmap=cmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f20bd812",
      "metadata": {
        "id": "f20bd812",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "*Notes:*  So, basically interesting factors that are affecting the mean factor are : rank, popularity, num_scoring_users, ignore num_list_users for now till further investigation of difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406981bb",
      "metadata": {
        "id": "406981bb",
        "outputId": "21fec757-1396-4c6e-c524-f9ceefffa356",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(data[pd.to_datetime(data['start_date']).dt.year >= 1980], x='start_date', color='media_type')\n",
        "fig.update_layout(bargap=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce618da",
      "metadata": {
        "id": "4ce618da",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "*Notes:* Obviously 2016 was a good year for Otakus :3 specially summer-Autumn-Fall seasons, with 119 tv, 45 movie, 23 ova, 61 ona, 60 special and 41 music. (Gotta check watching list lmao)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5007ddac",
      "metadata": {
        "id": "5007ddac",
        "outputId": "f15d39f1-7604-491e-95c8-5c82de7b805a",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data_main.groupby('num_episodes')['id'].count().sort_values(ascending=False).head(30).plot(kind='bar', figsize=(8,6))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553dbaad",
      "metadata": {
        "id": "553dbaad",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "*Notes:* A lot of Movies (1 episode) that's why the spike, but the summation of all others are the other percentages of tv,ova,ona,... etc. most tv/specials are short 12 (episodes)/(season|title). </br>\n",
        "*The fans of \"When you have eliminated the impossible\" teenager for 22+ years don't give up :(* </br>\n",
        "*Gomu Gomu no guys don't be Sadge :(*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa1fb06",
      "metadata": {},
      "outputs": [],
      "source": [
        "#data_main[['title','fav_percent']].sort_values(by=['fav_percent'],ascending=False).head(30).plot(kind='bar', figsize=(15,10))\n",
        "data_main.sort_values(by=['fav_percent'],ascending=False)[1:21].plot(kind='bar',x='title',y='fav_percent');"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ca851077",
      "metadata": {},
      "source": [
        "One Piece in the top as expected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feaeb556",
      "metadata": {
        "id": "feaeb556"
      },
      "source": [
        "###  NLP Pre-processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1ca7a94b",
      "metadata": {
        "id": "1ca7a94b",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Synopsis Keyword Analysis:\n",
        "*(NLP)* :\n",
        "* KeyBERT.\n",
        "* Spacy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d6c720",
      "metadata": {
        "id": "97d6c720",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "* Creating clean text, nouns and keywords from synopsis.\n",
        "* Separate in new df for data analysis.\n",
        "* Delete Syns entries from main df.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7aa8b92",
      "metadata": {
        "id": "a7aa8b92",
        "outputId": "31b70f93-a7fd-4b94-dd7c-c6bc46b9522b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "NLP = sp.load(\"en_core_web_lg\")\n",
        "TITLE = 'Death Note'\n",
        "key_model = KeyBERT()\n",
        "data_main = data_main[~data_main.title.duplicated(keep='first')]\n",
        "text = data_main[data_main['title'] == TITLE].synopsis.values[0]\n",
        "def clean_text(text):\n",
        "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
        "    text = text.replace('\\n', \"\").replace('\\r', \"\")\n",
        "    text = text.replace('', \"\")\n",
        "    text = re.sub('[^a-zA-Z]', \" \", str(text))\n",
        "    text = ' '.join(text.split())\n",
        "    text = text.lower()\n",
        "    doc = NLP(text)\n",
        "    return doc\n",
        "\n",
        "doc = clean_text(text)\n",
        "print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a792fe",
      "metadata": {
        "id": "75a792fe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data_main.loc[:,'cleaned_syn'] = data_main.loc[:,'synopsis'].astype(str).apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40453053",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_main.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da40e5b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_string=data_main[['title','synopsis','cleaned_syn']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86ce1d7",
      "metadata": {
        "id": "a86ce1d7",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "df_num=pd.get_dummies(data_main[[\"media_type\",\"source\",\"nsfw\",\"genres\",\"rating\",\"studios\"]], columns=[\"media_type\",\"source\",\"nsfw\",\"genres\",\"rating\",\"studios\"], prefix=[\"media_type\",\"source\",\"nsfw\",\"genres\",\"rating\",\"studios\"])\n",
        "df_num[['id','mean','num_scoring_users','num_episodes','popularity','fav_percent','rank']]=data_main[['id','mean','num_scoring_users','num_episodes','popularity','fav_percent','rank']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb998201",
      "metadata": {
        "id": "bb998201",
        "outputId": "3382e06a-26d9-4658-814f-77ac25325425",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Based on https://stackoverflow.com/questions/48925328/how-to-get-all-noun-phrases-in-spacy\n",
        "def get_candidates(doc):\n",
        "    # code to recursively combine nouns\n",
        "    # 'We' is actually a pronoun but included in your question\n",
        "    # hence the token.pos_ == \"PRON\" part in the last if statement\n",
        "    # suggest you extract PRON separately like the noun-chunks above\n",
        "\n",
        "    index = 0\n",
        "    noun_indices = [i for i, token in enumerate(doc) if token.pos_ == 'NOUN']\n",
        "    candidates = []\n",
        "    for idxValue in noun_indices:\n",
        "        start = doc[idxValue].left_edge.i if not bool(doc[idxValue].left_edge.ent_type_) else idxValue\n",
        "        finish = doc[idxValue].right_edge.i+1 if not bool(doc[idxValue].right_edge.ent_type_) else idxValue + 1\n",
        "        if 0 < finish-start < 7:\n",
        "            span = doc[start:finish]\n",
        "            candidates.append(span.text)\n",
        "    return candidates\n",
        "\n",
        "candidates = get_candidates(doc)\n",
        "print(candidates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d20d476",
      "metadata": {
        "id": "9d20d476",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### **Applying Key-BERT for Keywords extraction:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e49affa",
      "metadata": {
        "id": "9e49affa",
        "outputId": "6abc6482-4215-433d-aaae-813c0db7d7c3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "key_model = KeyBERT()\n",
        "def get_keywords(doc):\n",
        "    keywords = key_model.extract_keywords(doc.text,keyphrase_ngram_range=(1, 2), candidates=candidates,stop_words='english', use_mmr=True, diversity=0.7)\n",
        "    return keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fa69e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "get_keywords(doc)\n",
        "df_string.loc[:,'nouns'] = df_string.loc[:,'cleaned_syn'].apply(get_candidates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53034b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_string.loc[:,'keywords'] = df_string.loc[:,'cleaned_syn'].apply(get_keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50b2939",
      "metadata": {
        "id": "a50b2939",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "df_string.sample(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7236910f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_num.shape,df_string.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265a7dda",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_string.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218fb0ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_string.to_csv(r'M:\\Anime Recommender\\data-history\\up-to-date-MAL\\anime_string_latest.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c315b71f",
      "metadata": {},
      "source": [
        " `df_num` is the Data Frame for the Analytical approach and distance techniques. <br />\n",
        " `df_string` is the textual Data Frame for the NLP approaches to get contextual/semantic content based recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e24b29de",
      "metadata": {
        "id": "e24b29de",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# 2- **MODELS TIME:** :3\n",
        "![image info](https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/gettyimages-458406992-1538405221.jpg?crop=0.9xw:0.9xh;0,0&resize=256:*) <br />\n",
        "  July's 2022 Work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e80289",
      "metadata": {
        "id": "44e80289",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Similarity Analysis :"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e0c466a5",
      "metadata": {},
      "source": [
        "### Similarity Analysis Using The Numerical Features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "91d9f1d6",
      "metadata": {},
      "source": [
        "#### Model 1 :  Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a354946",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KDTree\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df_num.drop(['id'],axis=1).to_numpy())\n",
        "\n",
        "X = scaler.transform(df_num.drop(['id'],axis=1).to_numpy())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a81dd5e9",
      "metadata": {},
      "source": [
        "KDTree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e258982",
      "metadata": {},
      "outputs": [],
      "source": [
        "kdt = KDTree(X, metric='euclidean')\n",
        "indices = kdt.query(X, k=15, return_distance=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d9373d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_item_recommendations(anime_title, anime_idx=-1):\n",
        "    if anime_idx == -1:     \n",
        "        anime_idx = data_main[data_main['title'] == anime_title].index[0]\n",
        "    return data_main.iloc[indices[anime_idx][1:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf7bc480",
      "metadata": {},
      "outputs": [],
      "source": [
        "get_item_recommendations('Sword Art Online')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f39afdbd",
      "metadata": {},
      "source": [
        "### Similarity Analysis Using The String Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260b0e86",
      "metadata": {
        "id": "260b0e86",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f89b43",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "27289459ab0c422f8f071039bf1823a1"
          ]
        },
        "id": "28f89b43",
        "outputId": "f2781b46-f5b3-445c-eaed-344c95a34ded",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# import sweetviz as sv\n",
        "# #You could specify which variable in your dataset is the target for your model creation. We can specify it using the target_feat parameter.\n",
        "# data_report = sv.analyze(data_main)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d26d21",
      "metadata": {
        "id": "f1d26d21",
        "outputId": "067b8f8d-d220-4c16-9aee-f9527874ebf6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# data_report.show_notebook(w=1500, h=900, scale=0.8)\n",
        "# data_report.show_html(scale=0.9)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4d88668b",
      "metadata": {
        "id": "4d88668b",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### **Cos-similarity | TFIDF:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dedbd7a0",
      "metadata": {
        "id": "dedbd7a0",
        "outputId": "8bd6ef11-5688-4b35-9334-5c1d88d57671",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(data['synopsis'] + data['genres'] + data['rating'] + data['studios']+data['media_type'])\n",
        "tfidf_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08eac98b",
      "metadata": {
        "id": "08eac98b",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Using the Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies. \n",
        "\n",
        "$cosine(x,y) = \\frac{x. y^\\intercal}{||x||.||y||}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89470da8",
      "metadata": {
        "id": "89470da8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "cos_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d656c42",
      "metadata": {
        "id": "8d656c42",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data = data_main.reset_index()\n",
        "titles = data['title']\n",
        "indices = pd.Series(data_main.index, index=data['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd19b9d5",
      "metadata": {
        "id": "fd19b9d5",
        "outputId": "36e0d9af-9ffd-488d-8e6d-37f637a87f66",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_recommendations(title):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cos_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:31]\n",
        "    anime_indices = [i[0] for i in sim_scores]\n",
        "    return titles.iloc[anime_indices]\n",
        "data['title'][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e209620e",
      "metadata": {
        "id": "e209620e",
        "outputId": "6e312c15-4196-4c5a-8f8f-33998427c6e2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "cos_results=get_recommendations('Death Note').head(10)\n",
        "cos_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4106f59b",
      "metadata": {
        "id": "4106f59b",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Not so close recommendations but good start\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54189257",
      "metadata": {
        "id": "54189257",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**zenzen wakaranaaaaaiiiii !!!!!!!!!!!!!** </br>\n",
        ":\"D </br>\n",
        "pair-wise distance results not related to cosine Similarity results at all no intersections. </br>\n",
        "using keywords or using Full synopsis didn't matter for cos similarity so better for resources use keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c378393",
      "metadata": {
        "id": "7c378393",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### **RecommendNet Maybe?** :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0b26d0",
      "metadata": {
        "id": "7f0b26d0"
      },
      "source": [
        "**Zenzen heiki janai :\"D , Tasukete, Dare ka tasukeeteeeeee !** <br />\n",
        "  Aug 2022 Work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "247e6f40",
      "metadata": {
        "id": "247e6f40"
      },
      "source": [
        "#### *Normal Recommender features*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6871a874",
      "metadata": {
        "id": "6871a874",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "from tensorflow.python.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "start_lr = 0.00001\n",
        "min_lr = 0.00001\n",
        "max_lr = 0.00005\n",
        "batch_size = 10000\n",
        "rampup_epochs = 5\n",
        "sustain_epochs = 0\n",
        "exp_decay = .8\n",
        "\n",
        "def lrfn(epoch):\n",
        "    if epoch < rampup_epochs:\n",
        "        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "    elif epoch < rampup_epochs + sustain_epochs:\n",
        "        return max_lr\n",
        "    else:\n",
        "        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "\n",
        "\n",
        "lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)\n",
        "\n",
        "checkpoint_filepath = './weights.h5'\n",
        "\n",
        "model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                            save_weights_only=True,\n",
        "                            monitor='val_loss',\n",
        "                            mode='min',\n",
        "                            save_best_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(patience = 3, monitor='val_loss', \n",
        "                            mode='min', restore_best_weights=True)\n",
        "\n",
        "my_callbacks = [\n",
        "    model_checkpoints,\n",
        "    lr_callback,\n",
        "    early_stopping,   \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d533cdb7",
      "metadata": {
        "id": "d533cdb7",
        "outputId": "91aec886-4e2f-48ca-858b-3dceea5b0ab3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(len(X_test_array[0]))\n",
        "print(len(y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f0a848e",
      "metadata": {
        "id": "9f0a848e",
        "outputId": "e965f31a-b1ba-459b-87f9-887c6839476d",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Model training\n",
        "history = model1.fit(\n",
        "    x=X_train_array,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    validation_data=(X_test_array, y_test),\n",
        "    callbacks=my_callbacks\n",
        ")\n",
        "\n",
        "model1.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f21adab",
      "metadata": {
        "id": "1f21adab",
        "outputId": "7873c0fb-1e76-4d6c-f952-2bdd5dee5468",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#Training results\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(history.history[\"loss\"][0:-2])\n",
        "plt.plot(history.history[\"val_loss\"][0:-2])\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a05e281",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "927a943c968d4e0cb24c3746a0eedf41"
          ]
        },
        "id": "7a05e281",
        "outputId": "8ee593ce-3ee7-484d-ce1b-533d616a06aa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "\n",
        "history = model1.fit(\n",
        "    x=X_train_array,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=30,\n",
        "    validation_data=(X_test_array, y_test),\n",
        "    verbose = 0, \n",
        "    callbacks=[TqdmCallback(verbose=0)])\n",
        "\n",
        "model1.load_weights(checkpoint_filepath)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f87a7a83",
      "metadata": {
        "id": "f87a7a83",
        "outputId": "43567acc-f996-4573-e959-01b266b6d351",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#Training results\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(history.history[\"loss\"][0:-2])\n",
        "plt.plot(history.history[\"val_loss\"][0:-2])\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04064dde",
      "metadata": {
        "id": "04064dde",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def extract_weights(name, model):\n",
        "    weight_layer = model.get_layer(name)\n",
        "    weights = weight_layer.get_weights()[0]\n",
        "    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))\n",
        "    return weights\n",
        "\n",
        "anime_weights = extract_weights('anime_embedding', model1)\n",
        "user_weights = extract_weights('user_embedding', model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d50262",
      "metadata": {
        "id": "58d50262",
        "outputId": "4802fff6-b51f-4175-d116-d765076b1d2d",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data_main.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe5d7a7",
      "metadata": {
        "id": "bbe5d7a7",
        "outputId": "898e1150-d462-4e6b-db9d-c02ffbbf373e",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "name = data[data_main.id == 100].title.values[0]\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f800769",
      "metadata": {
        "id": "8f800769",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Fixing Names\n",
        "def get_animename(anime_id):\n",
        "    try:\n",
        "        name = data[data_main.id == anime_id].title.values[0]\n",
        "        return name\n",
        "    except:\n",
        "        print('error')\n",
        "        return 0\n",
        "\n",
        "data[\"eng_version\"] = data['title']\n",
        "\n",
        "\n",
        "data_main.sort_values(by=['mean'], \n",
        "                inplace=True,\n",
        "                ascending=False, \n",
        "                kind='quicksort',\n",
        "                na_position='last')\n",
        "\n",
        "df = data[[\"id\",\"title\", \"mean\", \"genres\", \"num_episodes\", \n",
        "        \"media_type\",\"synopsis\"]]\n",
        "\n",
        "\n",
        "def get_animeframe(anime):\n",
        "    if isinstance(anime, int):\n",
        "        return df[df.id == anime]\n",
        "    if isinstance(anime, str):\n",
        "        return df[df.title == anime]\n",
        "def get_sypnopsis(anime):\n",
        "    if isinstance(anime, int):\n",
        "        return df[df.id == anime].synopsis.values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9303a7fd",
      "metadata": {
        "id": "9303a7fd",
        "outputId": "1a303010-4a50-472a-c601-ce835de67b1a",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871b7023",
      "metadata": {
        "id": "871b7023",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"max_colwidth\", None)\n",
        "\n",
        "def find_similar_animes(name, n, return_dist=False, neg=False):\n",
        "        index = get_animeframe(name).id.values[0]\n",
        "        print(index)\n",
        "        encoded_index = anime2anime_encoded.get(index)\n",
        "        weights = anime_weights\n",
        "        print(encoded_index)\n",
        "        dists = np.dot(weights, weights[encoded_index])\n",
        "        sorted_dists = np.argsort(dists)\n",
        "        \n",
        "        n = n + 1            \n",
        "        \n",
        "        if neg:\n",
        "            closest = sorted_dists[:n]\n",
        "        else:\n",
        "            closest = sorted_dists[-n:]\n",
        "        print('animes closest to {}'.format(name))\n",
        "        if return_dist:\n",
        "            return dists, closest\n",
        "        rindex = df\n",
        "        similarityarr = []\n",
        "        for close in closest:\n",
        "            decoded_id = anime_encoded2anime.get(close)\n",
        "            sypnopsis = get_sypnopsis(decoded_id)\n",
        "            anime_frame = get_animeframe(decoded_id)\n",
        "            anime_name = anime_frame.title.values[0]\n",
        "            genre = anime_frame.genres.values[0]\n",
        "            similarity = dists[close]\n",
        "            similarityarr.append({\"id\": decoded_id, \"title\": anime_name,\n",
        "                            \"similarity\": similarity,\"genres\": genre,\n",
        "                            'synopsis': sypnopsis})\n",
        "        frame = pd.Dataframe(similarityarr).sort_values(by=\"similarity\", ascending=False)\n",
        "        return frame[frame.id != index].drop(['id'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a5a7329",
      "metadata": {
        "id": "8a5a7329",
        "outputId": "505adc9c-857f-49c8-8c55-7097a0980774",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "find_similar_animes('Death Note', n=10, neg=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a7edce",
      "metadata": {
        "id": "03a7edce"
      },
      "source": [
        "#### *Features modding* <br />\n",
        "   Modifying parameters for Recommend NET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efac23d8",
      "metadata": {
        "id": "efac23d8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# dfdl =pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5f6df5b",
      "metadata": {
        "id": "a5f6df5b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# dfdl_ids = data[\"id\"].tolist()\n",
        "# dfdlid_encoded = {x: i for i, x in enumerate(dfdl_ids)}\n",
        "# n_animes = len(dfdlid_encoded)\n",
        "# id_encoded2id = {i: x for i, x in enumerate(dfdl_ids)}\n",
        "# dfdl[\"id\"] = data[\"id\"].map(dfdlid_encoded)\n",
        "\n",
        "# dfdl_mean = data[\"mean\"].tolist()\n",
        "# dfdl_mean_encoded = {x: i for i, x in enumerate(dfdl_mean)}\n",
        "# mean_encoded2mean = {i: x for i, x in enumerate(dfdl_mean)}\n",
        "# n_users = len(dfdl_mean_encoded)\n",
        "# dfdl[\"mean\"] = data[\"mean\"].map(dfdl_mean_encoded)\n",
        "\n",
        "# dfdl_pop = data[\"popularity\"].tolist()\n",
        "# user2user_encoded = {x: i for i, x in enumerate(dfdl_pop)}\n",
        "# user_encoded2user = {i: x for i, x in enumerate(dfdl_pop)}\n",
        "# n_users = len(user2user_encoded)\n",
        "# dfdl[\"popularity\"] = data[\"popularity\"].map(user2user_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12296c98",
      "metadata": {
        "id": "12296c98",
        "outputId": "e529fa72-6a4d-4210-b465-739ccf70e4f2"
      },
      "outputs": [],
      "source": [
        "data_main.columns, data_main.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d82a86",
      "metadata": {
        "id": "01d82a86",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# x1 = rdf[['user', 'anime']].values \n",
        "\n",
        "# #x2=  data[['id'],['popularity']].values\n",
        "# x3=data[['mean'],['num_scoring_users']].values\n",
        "# x4=data['rank'].tolist(),data['num_favorites'].tolist()\n",
        "# x5= df.filter(regex='^media_type_',axis=1).values[i]\n",
        "# x6= df.filter(regex='^source_',axis=1).values[i]\n",
        "\n",
        "# y = rdf[\"rating\"]\n",
        "# # Split\n",
        "# test_set_size = 250000 #10k for test set\n",
        "# train_indices = rdf_sampled.shape[0] - test_set_size \n",
        "# len(x1),len(x2),len(x2[1]),len(x3),len(x3[1]),len(x4),len(x4[1]),len(y),\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef43819",
      "metadata": {
        "id": "2ef43819",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# X3= x3[:,0] + x3[:,2] +x3[:,3] + x3[:,4] + x3[:,5] + x3[:,1] \n",
        "# X4=['None']*len(x4)*len(x4[1])\n",
        "# for i in range(len(x4[1])):\n",
        "#     X4 =X4 + x4[:,i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e69923",
      "metadata": {
        "id": "45e69923",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# X1_train, X1_test, y_train, y_test = (\n",
        "#     x1[:train_indices],\n",
        "#     x1[train_indices:],\n",
        "#     y[:train_indices],\n",
        "#     y[train_indices:],\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2e04ac",
      "metadata": {
        "id": "ba2e04ac"
      },
      "source": [
        "### *After Reading Some Articels:*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073252b3",
      "metadata": {
        "id": "073252b3"
      },
      "source": [
        "#### **Research at home** <br />\n",
        "   Dec. 2022 work <br />\n",
        "\n",
        "Semantic Similarity on synopsis using nlp models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af4b48c3",
      "metadata": {
        "id": "af4b48c3"
      },
      "source": [
        "Potential Models for learning: <br />\n",
        "* paraphrase-miniLM\n",
        "* stsb-roberta latest alternatives\n",
        "* bert-base-nli-mean-tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da36912d",
      "metadata": {
        "id": "da36912d"
      },
      "source": [
        "**To_Do:**\n",
        "- Get embeddings from pretrained for all synopsis ( all paragraphs ).\n",
        "- Compare Similarity using distance wise / cosine / pairwise whatever the hell will measure similarity of embeddings.\n",
        "- Worst case senario ,(For each sentence embeddings in the requested anime synopsis loop cosine similarity between all sentences in all other synopsis)\n",
        "- Optimization worth testing: Finding similarity between sentences in the same synopsis to get unique sentences and store those while ignoring sentences that are pretty much similar in embeddings, that leads to having smaller group of sentences for each synopsis to loop on (Still looping bratan).\n",
        "- 5Head IDEA: Semantic Keyword embeddings similarity analysis to get potential chosen titles to do semantic sentence analysis on.\n",
        "- **OR JUST USE PARAPHRASE MINING U Fokin IDIOT, anata BAKA ??? hontoni BAKAAAA.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aeab76e7",
      "metadata": {},
      "source": [
        "## *Using Synopsis NLP:*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "16953489",
      "metadata": {},
      "source": [
        "#### revised pytorch April 23\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a38d609",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abb341fb",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "847913ee",
      "metadata": {},
      "source": [
        "#### Using the PyTorch universal encoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d2fc0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Pytorch implementation:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sentence_transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "from IPython.display import display\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f72812ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the GPU as the device for PyTorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee91a7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the UniversalSentenceEncoder model\n",
        "print(\"Loading model...\")\n",
        "model = sentence_transformers.SentenceTransformer('roberta-base')\n",
        "model = model.to(device)\n",
        "print(\"Loaded model and moved it to device.\")\n",
        "print(f\"Memory allocated: {torch.cuda.memory_allocated(device=device)/(1024**2):.2f} MB\")  # <-- add this line to check GPU memory usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7da3bcdf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the string data:\n",
        "df_string = pd.read_csv(r'M:\\Anime Recommender\\data-history\\up-to-date-MAL\\anime_string_latest.csv')\n",
        "df_string.rename(columns={\"Unnamed: 0\": \"ID\"}, inplace=True)\n",
        "df_string.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be30d152",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to get the text encoding using the GPU\n",
        "@torch.no_grad()\n",
        "def get_encoding(x):\n",
        "    if isinstance(x, str) and x.strip() != \"\":\n",
        "        print(f\"Encoding: {x}\", flush=True)\n",
        "        sys.stdout.flush()\n",
        "        encoding = torch.tensor(model.encode([x], show_progress_bar=True), device=device)\n",
        "        print(f\"Encoded tensor shape: {encoding.shape}\", flush=True)\n",
        "        sys.stdout.flush()\n",
        "        return encoding\n",
        "    else:\n",
        "        print(f\"Encoding: None\", flush=True)\n",
        "        sys.stdout.flush()\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "732792f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, df, device):\n",
        "        self.df = df\n",
        "        self.device = device\n",
        "        self.df['cleaned_syn_encoding'] = self.df['cleaned_syn'].apply(get_encoding)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.df['cleaned_syn_encoding'].iloc[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ec47e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from scipy.spatial import distance\n",
        "\n",
        "\n",
        "def get_top_similar(df , k):\n",
        "    # Extract the embeddings from the \"encodings\" column into a numpy array\n",
        "    embeddings = df['encodings'].to_numpy()\n",
        "    \n",
        "    # Compute pairwise cosine distances between all embeddings\n",
        "    distances = distance.pdist(embeddings, metric='cosine')\n",
        "    \n",
        "    # Convert condensed distance matrix to square distance matrix\n",
        "    dist_matrix = distance.squareform(distances)\n",
        "    \n",
        "    # Sort the distance matrix to get indices of the most similar embeddings for each embedding\n",
        "    sorted_indices = np.argsort(dist_matrix)\n",
        "    \n",
        "    # Create an empty list to hold the top \"n\" similar IDs for each embedding\n",
        "    top_n_similar_ids = []\n",
        "    \n",
        "    # Iterate through each row in the sorted_indices array and extract the top \"n\" IDs\n",
        "    for i in range(len(sorted_indices)):\n",
        "        top_n_similar_ids.append(list(df['ID'].iloc[sorted_indices[i][1:n+1]]))\n",
        "    \n",
        "    # Create a new DataFrame column called \"top_n_similar_ids\" containing the top \"n\" similar IDs for each embedding\n",
        "    df['top_n_similar_ids'] = top_n_similar_ids\n",
        "    \n",
        "    # Return the updated DataFrame\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d3728a",
      "metadata": {},
      "outputs": [],
      "source": [
        "sys.stdout.flush()\n",
        "dataset = MyDataset(df_string, device)\n",
        "dataloader = DataLoader(dataset, batch_size=64, num_workers=1, pin_memory=True)\n",
        "result = []\n",
        "for batch in dataloader:\n",
        "    with torch.cuda.amp.autocast():\n",
        "        encodings = torch.stack(batch)\n",
        "        print(f\"Memory allocated: {torch.cuda.memory_allocated(device=device)/(1024**2):.2f} MB\", flush=True)  # <-- add this line to check GPU memory usage\n",
        "        sys.stdout.flush()\n",
        "df_string['encodings'] = encodings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "979b79fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "closest_IDs = get_top_similar( df_string,k=15) \n",
        "df_string['similar_IDs'] = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cd7a89",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Define a function to get the top k most similar IDs using PyTorch\n",
        "def get_top_k_similar_IDs(x, df, k=15):\n",
        "    encoding = x\n",
        "    if encoding is not None:\n",
        "        print(\"Computing cosine similarity distances...\", flush=True)\n",
        "        sys.stdout.flush()\n",
        "        distances = F.cosine_similarity(encoding, torch.stack(df['cleaned_syn_encoding'].values))\n",
        "        print(f\"Distances tensor shape: {distances.shape}\", flush=True)\n",
        "        sys.stdout.flush()\n",
        "        closest_paragraphs_indices = distances.argsort(descending=True)[:k + 1][1:]\n",
        "        print(f\"Top {k} indices: {closest_paragraphs_indices}\", flush=True)\n",
        "        sys.stdout.flush()\n",
        "        return df.iloc[closest_paragraphs_indices]['ID'].values\n",
        "    else:\n",
        "        print(\"Encoding is None, returning None.\", flush=True)\n",
        "        sys.stdout.flush()\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# Define the function to process each chunk\n",
        "def process_chunk(chunk):\n",
        "    print(f\"Processing chunk {chunk.index[0]}-{chunk.index[-1]}...\", flush=True)\n",
        "    sys.stdout.flush()\n",
        "    dataset = MyDataset(chunk, device)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, num_workers=4, pin_memory=True)\n",
        "    result = []\n",
        "    for batch in dataloader:\n",
        "        with torch.cuda.amp.autocast():\n",
        "            encodings = torch.stack(batch)\n",
        "            print(f\"Memory allocated: {torch.cuda.memory_allocated(device=device)/(1024**2):.2f} MB\", flush=True)  # <-- add this line to check GPU memory usage\n",
        "            sys.stdout.flush()\n",
        "            closest_IDs = [get_top_k_similar_IDs(encodings[i], chunk, k=15) for i in range(len(batch))]\n",
        "        result.extend(closest_IDs)\n",
        "    chunk['similar_IDs'] = result\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated(device=device)/(1024**2):.2f} MB\", flush=True)  # <-- add this line to check GPU memory usage\n",
        "    sys.stdout.flush()\n",
        "    return chunk\n",
        "def init_child(model_, device_):\n",
        "    print(\"Initializing child process with model and device...\", flush=True)\n",
        "    sys.stdout.flush()\n",
        "    global model, device\n",
        "    model = model_\n",
        "    device = device_\n",
        "    print(\"Initialized child process with model and device.\", flush=True)\n",
        "    sys.stdout.flush()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b0de510",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1463f8b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_string.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a3b828",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_string.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa049907",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the number of processes to use\n",
        "num_processes = torch.multiprocessing.cpu_count()\n",
        "print(f\"Using {num_processes} processes.\")\n",
        "# Split the data into chunks\n",
        "chunks = np.array_split(df_string, num_processes)\n",
        "print(f\"Data split into {len(chunks)} chunks.\")\n",
        "# Set up the multiprocessing pool\n",
        "print(\"Initializing multiprocessing pool...\")\n",
        "pool = torch.multiprocessing.Pool(processes=num_processes, initializer=init_child, initargs=(model, device))\n",
        "print(\"Multiprocessing pool created.\", flush=True)\n",
        "# Process the chunks in parallel\n",
        "results = []\n",
        "start_time = time.time()\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1}/{len(chunks)}...\", flush=True)\n",
        "    sys.stdout.flush()\n",
        "    result = process_chunk(chunk)\n",
        "    results.append(result)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Chunk {i+1}/{len(chunks)} processed in {elapsed_time:.2f} seconds.\", flush=True)\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated(device=device)/(1024**2):.2f} MB\", flush=True)\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "# Combine the results from all processes\n",
        "final_result = pd.concat(results)\n",
        "print(\"Results combined.\")\n",
        "# Close the pool\n",
        "pool.close()\n",
        "pool.join()\n",
        "print(\"Pool closed.\")\n",
        "# Save the final result to a CSV file\n",
        "final_result.to_csv('similar_animes.csv', index=False)\n",
        "print(\"Result saved to file.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c8744a47",
      "metadata": {},
      "source": [
        "still taking so long time \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd690faa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "import cProfile\n",
        "import pstats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee26dc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_string = pd.read_csv(r'M:\\Anime Recommender\\data-history\\up-to-date-MAL\\anime_string_encodings_bert.csv',index_col=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8951fa3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "type(df_string['cleaned_syn_encoding'].values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e60627a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_string.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd806c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df_string.sample(1000)\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "364579c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_k_similar_paragraphs(x,df, k=10):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import faiss\n",
        "    encoding = x\n",
        "    if encoding is not None:\n",
        "        query_embedding = np.array(encoding, dtype=np.float32)\n",
        "        corpus_embeddings = np.stack(df['cleaned_syn_encoding'].values)\n",
        "\n",
        "        # Use Faiss to index the corpus embeddings\n",
        "        index = faiss.IndexFlatIP(query_embedding.size)\n",
        "        index.add(corpus_embeddings)\n",
        "\n",
        "        # Search the index to get the top k similar paragraphs\n",
        "        distances, indices = index.search(np.array([query_embedding]), k+1)\n",
        "        indices = indices.squeeze()[1:]\n",
        "        \n",
        "        return df.loc[indices, 'ID'].values\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1c30024",
      "metadata": {},
      "outputs": [],
      "source": [
        "float(df['cleaned_syn_encoding'].values[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c2d102",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "198726d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandarallel import pandarallel\n",
        "import re\n",
        "pandarallel.initialize(progress_bar=True)\n",
        "\n",
        "# # df.apply(func)\n",
        "# match_number = re.compile('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
        "# x=  [float(x) for x in re.findall(match_number, string(df['cleaned_syn_encoding'].to_list)] \n",
        "\n",
        "df['similar_paragraphs'] = df['cleaned_syn_encoding'].parallel_apply( get_top_k_similar_paragraphs, df =df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92aac3c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Break down the data into smaller chunks\n",
        "chunk_size = 5\n",
        "chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "\n",
        "# Define the function to process each chunk\n",
        "def process_chunk(chunk):\n",
        "    chunk['similar_paragraphs'] = chunk['cleaned_syn_encoding'].apply(lambda x: get_top_k_similar_paragraphs(x, df))\n",
        "    return chunk\n",
        "\n",
        "# Create a Pool with the number of processes equal to the number of chunks\n",
        "with Pool(len(chunks)) as p:\n",
        "    cProfile.runctx(\"result = p.map(process_chunk, chunks)\", globals(), locals(), \"Profile.prof\")\n",
        "    \n",
        "# Concatenate the results\n",
        "df = pd.concat(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b345cf48",
      "metadata": {},
      "outputs": [],
      "source": [
        "p = pstats.Stats(cProfile)\n",
        "p.strip_dirs().sort_stats(\"time\").print_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e36c118",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e416f5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_similar_animes(title, top_n=5):\n",
        "    index = df_string[df_string['title'] == title].index[0]\n",
        "    similar_indices = df_string['similar_paragraphs'][index][:top_n]\n",
        "    similar_animes = df_string.iloc[similar_indices]['title'].values\n",
        "    return similar_animes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d686369",
      "metadata": {},
      "outputs": [],
      "source": [
        "title = \"Naruto\"\n",
        "top_similar_animes = get_top_similar_animes(title, df_string, top_n=10)\n",
        "print(top_similar_animes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4bbdb20e",
      "metadata": {},
      "source": [
        "It took so long lets find out why ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18382b53",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import cProfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8eb1b45",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the function to process each chunk\n",
        "def process_chunk(chunk):\n",
        "    df['similar_paragraphs'] = df['cleaned_syn_encoding'].apply(lambda x: get_top_k_similar_paragraphs(x, df))\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c1d228",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a Pool with the number of processes equal to the number of chunks\n",
        "with Pool(2) as p:\n",
        "    cProfile.runctx(\"result = p.map(process_chunk, df)\", globals(), locals(), \"Profile.prof\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c1f34e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pstats\n",
        "\n",
        "p = pstats.Stats(\"profile_results\")\n",
        "p = pstats.Stats(cProfile)\n",
        "p.strip_dirs().sort_stats(\"time\").print_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233c08dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "# set the GPU as the device for TensorFlow\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61278db8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the string data:\n",
        "df_string=pd.read_csv(r'M:\\Anime Recommender\\data-history\\up-to-date-MAL\\anime_string_latest.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b2a5247",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_string.sample(3).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb310bc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "class UniversalSentenceEncoder:\n",
        "    def __init__(self, dimension=512):\n",
        "        self.dimension = dimension\n",
        "        try:\n",
        "            self.model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Input(shape=(1,), dtype=tf.string),\n",
        "                hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                output_shape=[self.dimension],\n",
        "                                trainable=False)\n",
        "            ])\n",
        "        except Exception as e:\n",
        "            print(\"An error occurred while creating the UniversalSentenceEncoder model:\", e)\n",
        "            self.model = None\n",
        "    def __call__(self, x):\n",
        "        if self.model is not None:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            print(\"The UniversalSentenceEncoder model is not available for use.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c69261cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "class UniversalSentenceEncoder:\n",
        "    def __init__(self, dimension=512):\n",
        "        self.dimension = dimension\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input(shape=(1,), dtype=tf.string),\n",
        "            tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=0)),\n",
        "            hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                            output_shape=[self.dimension],\n",
        "                            trainable=False)\n",
        "        ])\n",
        "    def __call__(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96800906",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the encoding model\n",
        "encoding_model = UniversalSentenceEncoder(512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84166cfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_encoding(model, x):\n",
        "    return model(tf.constant(x)).numpy().flatten()\n",
        "\n",
        "def get_cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def get_top_k_similar_paragraphs(x, df, encoding_model, k=15):\n",
        "    encoding = get_encoding(encoding_model, [x])\n",
        "    distances = np.array([get_cosine_similarity(encoding, vec) for vec in df_string['cleaned_syn_encoding']])\n",
        "    closest_paragraphs_indices = np.argsort(-distances)[:k + 1][1:]\n",
        "    return df_string.iloc[closest_paragraphs_indices]['cleaned_syn']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5803779b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the encodings for the paragraphs in the 'cleaned_syn' column\n",
        "df_string.loc[:,'cleaned_syn_encoding'] = df_string.loc[df_string['cleaned_syn'] != '', 'cleaned_syn'].apply(lambda x: get_encoding(encoding_model, [x]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83c123f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a new column 'similar_paragraphs' containing the top k similar paragraphs for each paragraph in the 'cleaned_syn' column\n",
        "df_string.loc[df_string['cleaned_syn'] != '', 'similar_paragraphs'] = df_string.loc[df_string['cleaned_syn'] != '', 'cleaned_syn'].apply(lambda x: get_top_k_similar_paragraphs(x, df, encoding_model))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "53b5bf601465e97d3bc26103c3f6e93ae804cb5db8486c47b1991b59c7b6e7bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
