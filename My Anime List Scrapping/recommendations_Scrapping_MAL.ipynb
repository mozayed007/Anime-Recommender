{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_url = 'https://api.myanimelist.net/v2'\n",
    "\n",
    "# A Client ID is needed (https://myanimelist.net/apiconfig)\n",
    "#with open('client_id.txt', 'r') as f:\n",
    "#    CLIENT_ID = f.read()\n",
    "\n",
    "headers = {'X-MAL-CLIENT-ID': \"ec1c4b7aa5f59e854390b2259667340c\"}\n",
    "\n",
    "def get_data(endpoint, params=None):\n",
    "    url = api_url + endpoint\n",
    "    if params:\n",
    "        url += '?' + '&'.join(f'{key}={value}' for key, value in params.items())\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "scraping_save_pages = '../My Anime List Scrapping/data/data_tmp/recomm_pages'\n",
    "\n",
    "if not os.path.exists(scraping_save_pages): # Create saving directory if it doesn't exist\n",
    "    os.makedirs(scraping_save_pages)\n",
    "endpoint_ranking = f'/anime/ranking'\n",
    "limit = 500\n",
    "anime_keys = ['id', 'title']\n",
    "\n",
    "def scrape_page(page):\n",
    "    params = {'ranking_type': 'bypopularity', 'limit': limit, 'offset': page*limit, 'fields': ','.join(anime_keys)}\n",
    "    data = get_data(endpoint_ranking, params)\n",
    "    useful = [anime['node'] for anime in data['data']]\n",
    "    with open(scraping_save_pages + f'/page{str(page).zfill(2)}.json', 'w') as f:\n",
    "        json.dump(useful, f, indent=4)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 22 April 2023\n",
    "previous_total_anime = 50_900\n",
    "previous_last_page = math.ceil(previous_total_anime / limit) - 1\n",
    "\n",
    "data = get_data(endpoint_ranking, {'ranking_type': 'bypopularity', 'limit': limit, 'offset': previous_last_page*limit, 'fields': ','.join(anime_keys)})\n",
    "#data = get_data(endpoint,{'ranking_type': 'bypopularity', 'limit': limit, 'offset': previous_last_page*limit, 'fields': ','.join(str(v) for v in 'recommendations')})\n",
    "#assert len(data['data']) > 0\n",
    "assert 'next' not in data['paging']\n",
    "\n",
    "\n",
    "last_page = previous_last_page\n",
    "\n",
    "last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "scraping_save_pages = '../My Anime List Scrapping/data/data_tmp/recomm_pages'\n",
    "\n",
    "if not os.path.exists(scraping_save_pages): # Create saving directory if it doesn't exist\n",
    "    os.makedirs(scraping_save_pages)\n",
    "\n",
    "api_url = 'https://api.myanimelist.net/v2'\n",
    "headers = {'X-MAL-CLIENT-ID': \"ec1c4b7aa5f59e854390b2259667340c\"}\n",
    "endpoint = f'/anime/'\n",
    "limit = 500\n",
    "anime_keys = ['id', 'title','recommendations']\n",
    "scraping_save_pages = '../My Anime List Scrapping/data/data_tmp/recomm_pages'\n",
    "ecount=0\n",
    "def scrape_page(page):\n",
    "    params = { 'fields': ','.join(anime_keys)}\n",
    "    for anime_id in range(1, 100000):\n",
    "        try:\n",
    "            data = get_recommendations(endpoint,anime_id,params)\n",
    "            useful = [anime['node'] for anime in data['data']]\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            ecount+=1\n",
    "    print(ecount + 'errors not exisitng IDs')\n",
    "    with open(scraping_save_pages + f'/page{str(page).zfill(2)}.json', 'w') as f:\n",
    "        json.dump(useful, f, indent=4)\n",
    "def get_recommendations(endpoint,anime_id, params=None):\n",
    "    url=api_url + endpoint\n",
    "    if params:\n",
    "        url += f'anime_id' + '?' + '&'.join(f'{key}={value}' for key, value in params.items())\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 22 April 2023\n",
    "previous_total_anime = 50_900\n",
    "previous_last_page = math.ceil(previous_total_anime / limit) - 1\n",
    "for anime_id in range(100, 100000):\n",
    "        try:\n",
    "            data = get_recommendations(endpoint,anime_id,{'fields': ','.join(anime_keys)})\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            ecount+=1\n",
    "            print(ecount)\n",
    "print(ecount + 'errors not exisitng IDs')\n",
    "assert 'next' not in data['paging']\n",
    "last_page = previous_last_page\n",
    "\n",
    "last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "for page in tqdm.trange(last_page+1):\n",
    "    scrape_page(page)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "scraping_save_pages = 'data/data_tmp/recomm_pages'\n",
    "\n",
    "data = []\n",
    "for file_name in os.listdir(scraping_save_pages):\n",
    "    file_path = os.path.join(scraping_save_pages, file_name)\n",
    "    with open(file_path, 'r') as f:\n",
    "        file = json.load(f)\n",
    "    data.extend(file)\n",
    "\n",
    "len(data),type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data),len(data[1])\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data_tmp/recomm_raw.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open('data/data_tmp/recomm_raw.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "anime = pd.json_normalize(data, sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
